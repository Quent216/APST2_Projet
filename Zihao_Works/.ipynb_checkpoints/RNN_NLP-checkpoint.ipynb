{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auteur:\n",
    "Sarra Mars & Zihao GUO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "UuQIqRgJtkXn",
    "outputId": "e45f9369-2c5a-4dd0-ec1e-e21dd9c20869"
   },
   "outputs": [],
   "source": [
    "import keras as ks\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWn4PPpptkX0"
   },
   "source": [
    "# Pré-processing de textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COx4XiS3tkX1"
   },
   "source": [
    "Cette [page](https://keras.io/preprocessing/text/) détaille les méthodes de pré-processing de texte avec Keras et présente notamment la classe Tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ia2UwTetkX2"
   },
   "source": [
    "> Completer le code ci-dessous pour créer un analyseur lexical (tokenizer) avec keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zllsV--itkX3"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['je suis un étudiant de Nantes.',\n",
    "           'Je ne manque jamais les cours de machine learning!',\n",
    "           'je suis étudiant à Centrale', \n",
    "           'je suis jeune', \n",
    "           'je mange pasta']\n",
    "\n",
    "### ne conserver que 1000 mots dans le corpus :\n",
    "mon_tokenizer = Tokenizer(num_words=1000, \n",
    "                filters='\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n') \n",
    "\n",
    "mon_tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDRUwZ86tkX8"
   },
   "source": [
    "> Quel est l'index du mot \"machine\" dans cet encodage ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "S9UuWgdRtkX9",
    "outputId": "0bf380e3-8d7c-46cb-8ae1-bb187e26b45e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'je': 1, 'suis': 2, 'étudiant': 3, 'de': 4, 'un': 5, 'nantes': 6, 'ne': 7, 'manque': 8, 'jamais': 9, 'les': 10, 'cours': 11, 'machine': 12, 'learning!': 13, 'à': 14, 'centrale': 15, 'jeune': 16, 'mange': 17, 'pasta': 18}\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "print(mon_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNLZI-mFtkYE"
   },
   "source": [
    "> Afficher la liste des termes de ponctuations qui sont retirés par le Tokenizer. Modifier le fitre pour ne pas retirer le point d'exclamation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "4n2xQJJttkYH",
    "outputId": "075c7a92-8928-4f87-f4c9-6a5bf8acfec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "print(mon_tokenizer.filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35kXP_2itkYP"
   },
   "source": [
    "> Transformer maintenant les mots en listes d'entiers avec la méthode `texts_to_sequences()` de la classe Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E6StTuVStkYQ",
    "outputId": "e75aeb58-59cb-452c-d897-abf81efd7d56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 5, 3, 4, 6], [1, 7, 8, 9, 10, 11, 4, 12, 13], [1, 2, 3, 14, 15], [1, 2, 16], [1, 17, 18]]\n"
     ]
    }
   ],
   "source": [
    "sequences = mon_tokenizer.texts_to_sequences(samples)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHFrtLqBtkYX"
   },
   "source": [
    "# Word Embeddings (plongement des mots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7rcQuoztkYY"
   },
   "source": [
    "Il existe deux façons d'obtenir des embeddings de mots:\n",
    "\n",
    "- On peut apprendre un plongement pour une tache bien précise en amont (comme la classification des documents ou la prédiction des sentiments). Dans ce cas, on apprend le plongement comme on le fait pour un réseau de neurone classique.\n",
    "\n",
    "- On peut utiliser un embedding qui a été pré-entrainé pour une autre tâche, et que l'on \"recycle\" ici pour représenter les mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FDM9jQXtkYZ"
   },
   "source": [
    "### Apprentissage du plongement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpFEBhphtkYc"
   },
   "source": [
    "> En consultant la documentation sur la couche [`Embedding` de Keras](https://keras.io/layers/embeddings/), indiquer quels paramètres faut-il donner en argument à la fonction `Embedding` pour que celle-ci puisse représenter un plongement dans un espace de dimension 64 de séquences de longeur 10 mots dans corpus de 1000 mots retenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "DiMZrn9GtkYd",
    "outputId": "1f4aa5d7-43bd-4467-8089-cc7a9d4bc10d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64, input_length=10)### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXrINO1CtkYh"
   },
   "source": [
    "La couche `Embedding` prend en entrée un tenseur 2D d'entiers, de taille  `nombre de séquences` x  `longueur d'une séquence`.\n",
    "\n",
    "Toutes les séquences dans un bacth (de séquences) doivent donc avoir la même longueur, quitte à tronquer ou compléter avec des zeros les séquences trop longues ou trop courtes.\n",
    "\n",
    "Cette couche renvoie un tenseur 3D de valeurs numériques de taille `nombre de séquences` x  `longueur d'une séquence` x `dim d'arrivée du plongement`. \n",
    "\n",
    "Ces tenseurs 3D  peuvent ensuite être connectés à des couches récurrentes ou convolutionnelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgKde-WgtkYh"
   },
   "source": [
    "Dans un réseau de neurones, nous allons maintenant créer une première couche de plongement (embedding layer) et nous allons apprendre les poids de ce plongement exactement comme on le fait pour une couche dense.  Nous allons pour cela utiliser les données [imdb newswires Reuters](https://keras.io/datasets/#reuters-newswire-topics-classification) qui peuvent être directement chargées dans keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yKRnp7gMtkYi",
    "outputId": "4830d804-7b61-4633-dad5-3653c86d4970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "max_mots = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_mots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XSpeleYtkYl"
   },
   "source": [
    "L'argument `num_words` correspond au nombre maximal de mots utilisés comme features. On le limite ici à 10000.\n",
    "\n",
    "> Vérifier que les mots ont été chargés sous la forme d'entiers. Que représente ici y ? Quel est l'objectif de ce problème d'apprentissage ? On parle \"d'analyse de sentiment\" (sentiment analysis ou opinion mining). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L3LuqLF3tkYl",
    "outputId": "072c5639-0872-4b72-c80a-bd7a5ab4d860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re: Representer les commentaires écrites prédire si une commentaire représente une émotion positive ou négitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYmkYuVrtkYo"
   },
   "source": [
    "> Bonus : retrouver les phrases à partir des vecteurs d'entiers (voir la doc de `imdb.load_dat`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTPAayRttkYo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 0s 0us/step\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "index_from=3\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+index_from) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in x_train[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0W8nqgotkYs"
   },
   "source": [
    "> Utiliser la fonction [preprocessing.sequence.pad_sequences](https://keras.io/api/preprocessing/timeseries/#padsequences-function) pour transformer `x_train` et `x_test` en deux tenseurs 2D de tailles `nb de sequences` x  `long max d une sequence = 20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s0S7qsz-tkYs",
    "outputId": "1f0bd33d-cf99-4824-eda7-aa7ec1ec25e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 20)\n",
      "(25000, 20)\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train_padded = pad_sequences(x_train, maxlen=20)\n",
    "x_test_padded = pad_sequences(x_test, maxlen=20)\n",
    "\n",
    "print (x_train_padded.shape)\n",
    "print (x_test_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raVGkng6tkYu"
   },
   "source": [
    "> Construire un réseau à propagation avant comme suit:\n",
    "- Une couche d'embedding qui plonge chaque mot dans un espace de dimension 8.\n",
    "- Une couche Flatten pour redimensionner le tenseur 3D des plongements en un tenseur 2D  de taille `nb de sequences` x  (8*20)  \n",
    "- Une couche dense avec activation sigmoid pour la classification finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "0Br819HBtkYv",
    "outputId": "50541480-1a21-4506-8bd1-e76adb38c9e8"
   },
   "outputs": [],
   "source": [
    "### TO DO ###\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.models import Model\n",
    "\n",
    "input = Input(shape=(20, ))\n",
    "x = Embedding(max_mots, 8, input_length=20)(input)\n",
    "x = Flatten()(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=input, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-RejtiAtkYx"
   },
   "source": [
    "> Utiliser un optimiseur `rmsprop` avec perte `binary_crossentropy` et suivi de la métrique `acc` (précision) le long de la trajectoire d'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "GZV3J0HHtkYy",
    "outputId": "6187d5de-f5c0-4b3a-ac34-059823397a45"
   },
   "outputs": [],
   "source": [
    "### TO DO ###\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1C5JLdxtkY0"
   },
   "source": [
    "> Affichez le résumé du réseau de neurones ainsi construit et assurez-vous que vous comprenez les dimensions affichées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "h7jle7NftkY0",
    "outputId": "ce7d01d1-707b-440b-d746-f71faab4d5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3bzGP3AtkY2"
   },
   "source": [
    "> Ajuster le modèle sur les données d'apprentissage et donner la précision de validation finale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "GABhvI-AtkY2",
    "outputId": "0a13e1a6-91c7-47e3-9109-a9ac944b53ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 10s 10ms/step - loss: 0.6600 - acc: 0.6313 - val_loss: 0.5890 - val_acc: 0.7183\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.5180 - acc: 0.7519 - val_loss: 0.4988 - val_acc: 0.7531\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.4520 - acc: 0.7892 - val_loss: 0.4802 - val_acc: 0.7640\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.4224 - acc: 0.8068 - val_loss: 0.4750 - val_acc: 0.7678\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.4006 - acc: 0.8204 - val_loss: 0.4759 - val_acc: 0.7702\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3825 - acc: 0.8298 - val_loss: 0.4797 - val_acc: 0.7700\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.3654 - acc: 0.8416 - val_loss: 0.4834 - val_acc: 0.7680\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.3491 - acc: 0.8506 - val_loss: 0.4882 - val_acc: 0.7670\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3332 - acc: 0.8584 - val_loss: 0.4959 - val_acc: 0.7667\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3181 - acc: 0.8680 - val_loss: 0.5030 - val_acc: 0.7638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1317897c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TO DO ###\n",
    "model.fit(x_train_padded, \n",
    "          y_train, \n",
    "          epochs=10, \n",
    "          batch_size=32, \n",
    "          validation_data=(x_test_padded,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nJhiFCHYtkY5"
   },
   "source": [
    "Le taux de bien classés tourne autour de 75%, ce qui est correct, mais on peut espérer faire mieux en utilisant le caractère \"séquentiel\" des phrases, grâce à des réseaux récurrents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlRRtQBftkY5"
   },
   "source": [
    "# Construction d'un réseau récurrent simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "339UmcyytkY6"
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9m0qT6d4tkY7"
   },
   "source": [
    "Une couche `SimpleRNN` prend en entrée un tenseur 3D de taille `batch_size` x `timesteps` (longeur de la séquence) x  `input_features` (typiquement la dimension de l'embedding). \n",
    "\n",
    "Comme tous les modèles récurrents, `SimpleRNN` peut renvoyer la suite complète de toutes les sorties pour chaque temps (le long de la séquence), ou bien tout simplement la denière sortie pour chaque séquence. \n",
    "\n",
    "> Expliquer la différence de dimension observée sur la couche récurrente dans les deux architectures proposées ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "zxzlwjjMtkY8",
    "outputId": "21751960-988e-4f4a-a686-7672e6466ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 10, 32)            32000     \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,080\n",
      "Trainable params: 34,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = 1000,output_dim=32,input_length = 10))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "4e9H80STtkY9",
    "outputId": "c02abeeb-81cb-4871-bf88-38ae0f4407c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 10, 32)            32000     \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 10, 32)            2080      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,080\n",
      "Trainable params: 34,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = 1000,output_dim=32,input_length = 10))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re: Différence : retourner la dernière sortie de la séquence de sortie, ou la séquence complète."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URasM4qDtkY_"
   },
   "source": [
    "> Préparer des données lexicales d'apprentissage et de test pour les données [`imdb`](https://keras.io/api/datasets/imdb/#load_data-function) selon les spécifications suivantes:\n",
    "- nombre de mots pris en compte : 10000 \n",
    "- longeur maximale des séquences : 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "L8R_jhZAtkY_",
    "outputId": "15be8d17-b515-4f86-b34b-f8a2651ca1f8"
   },
   "outputs": [],
   "source": [
    "### TO DO ###\n",
    "max_mots = 10000\n",
    "max_len = 500\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_mots, maxlen = max_len)\n",
    "x_train_padded = pad_sequences(x_train, maxlen=500)\n",
    "x_test_padded = pad_sequences(x_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwjsjJgdtkZB"
   },
   "source": [
    "> Construire un réseau à propagation avant comme suit:\n",
    "- Une couche d'embedding qui plonge chaque mot dans un espace de dimension 32.\n",
    "- Une couche `SimpleRNN` avec uniquement sortie finale\n",
    "- Une couche dense avec activation sigmoid pour la classification finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV0RW75ttkZC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 1)                 34        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,036\n",
      "Trainable params: 320,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_mots ,output_dim=32, input_length = max_len))\n",
    "model.add(SimpleRNN(1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNn_c4LMtkZD"
   },
   "source": [
    "> Utiliser un optimiseur `rmsprop` avec perte `binary_crossentropy` et suivi de la métrique `acc` (précision) le long de la trajectoire d'optimisation. Ajuster le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cbUbqektkZE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "340/716 [=============>................] - ETA: 2:01 - loss: 0.6898 - acc: 0.5300"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### TO DO ###\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ndev/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/ndev/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/ndev/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/ndev/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/ndev/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/ndev/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ndev/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/ndev/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/ndev/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train_padded, y_train, epochs=5, batch_size=32, validation_data=(x_test_padded,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5W0vrlNGwBgg"
   },
   "source": [
    "# Construction d'un réseau récurrent avec cellules LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_dK3HRtwhak"
   },
   "source": [
    "> Construire enfin un réseau similaire où vous aurez remplacé la couche SimpleRNN par une couche [LSTM](https://keras.io/layers/recurrent/#lstm).\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "6nSBWWW2wgv1",
    "outputId": "e7d7eeb6-09d9-4d17-fc4c-315807b9dffe"
   },
   "outputs": [],
   "source": [
    "### TO DO ###\n",
    "from keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_mots ,output_dim=32, input_length = max_len))\n",
    "model.add(LSTM(1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train_padded, y_train, epochs=1, batch_size=32, validation_data=(x_test_padded,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06AUej2ttkZU"
   },
   "source": [
    "# Utilisation d'un embedding pré-entrainé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DI1PFMdtkZV"
   },
   "source": [
    "Cette fois nous allons partir des données `Imdb` brutes et plonger celles-ci dans un espace via un plongement qui a déjà été ajusté (sur des données différentes et pour un problème autre). Nous allons utiliser [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove) : télécharger glove.6B.zip (près d'un giga !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Wa3SLdDtkZV"
   },
   "source": [
    "> Télécharger les données brutes à [cette adresse](http://mng.bz/0tIo). Les textes positifs et négatifs sont classés dans des repertoires de même nom. Compléter le code ci-dessous pour importer et préparer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tmklq8VptkZV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = ### TO DO ###\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in [### TO DO ###]:\n",
    "    dir_name = os.path.join(train_rep, ### TO DO ###)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, ### TO DO ###))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == ### TO DO ###:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJFHlNCItkZX",
    "outputId": "59fa292a-4360-490d-95c0-773eb3d9b566"
   },
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Px8rgO_TtkZa"
   },
   "source": [
    "> Effectuer les opérations de traitement lexical (tokenization) pour un corpus de 10000 mots et des séquence de mots d'une longueur maximale de 100 mots. Transformer `labels` en un vecteur `numpy`. Vérifier les dimensions des objets construits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rZyddzitkZt",
    "outputId": "2b8cc3f2-6a54-438b-f407-4d055b24f9e3"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM1r5wmctkZw"
   },
   "source": [
    "> Extraire 1000 données pour l'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FH37Eu7tkZw"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeltkAkhtkZx"
   },
   "source": [
    "> En suivant la documentation de Keras sur cette [page](https://keras.io/examples/nlp/pretrained_word_embeddings/), utiliser  un embedding de type Glove sur les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUdFEwuStkZy"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwVzJ3NptkZ0"
   },
   "source": [
    "> Utiliser cet embedding pour construire des réseaux recurrents ou non pour prédire la sortie Y. Evaluer la précision de vos modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8lsAZQVtkZ0"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDRxcABAtkZ5"
   },
   "source": [
    "> Utiliser cet embedding de mots pour évaluer la proximité entre quelques phrases que vous choisirez. Vous pourrez représenter les données dans le premier plan factoriel d'une ACP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRzZUbSptkZ6"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN-NLP_correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7673fce31fe4afe50b4d7357ba4e3f000f877c9b2250741ca11e5d63029f695"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
