{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auteur:\n",
    "Sarra Mars & Zihao GUO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "UuQIqRgJtkXn",
    "outputId": "e45f9369-2c5a-4dd0-ec1e-e21dd9c20869"
   },
   "outputs": [],
   "source": [
    "import keras as ks\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWn4PPpptkX0"
   },
   "source": [
    "# Pré-processing de textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COx4XiS3tkX1"
   },
   "source": [
    "Cette [page](https://keras.io/preprocessing/text/) détaille les méthodes de pré-processing de texte avec Keras et présente notamment la classe Tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ia2UwTetkX2"
   },
   "source": [
    "> Completer le code ci-dessous pour créer un analyseur lexical (tokenizer) avec keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zllsV--itkX3"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['je suis un étudiant de Nantes.',\n",
    "           'Je ne manque jamais les cours de machine learning!',\n",
    "           'je suis étudiant à Centrale', \n",
    "           'je suis jeune', \n",
    "           'je mange pasta']\n",
    "\n",
    "### ne conserver que 1000 mots dans le corpus :\n",
    "mon_tokenizer = Tokenizer(num_words=1000, \n",
    "                filters='\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n') \n",
    "\n",
    "mon_tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDRUwZ86tkX8"
   },
   "source": [
    "> Quel est l'index du mot \"machine\" dans cet encodage ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "S9UuWgdRtkX9",
    "outputId": "0bf380e3-8d7c-46cb-8ae1-bb187e26b45e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'je': 1, 'suis': 2, 'étudiant': 3, 'de': 4, 'un': 5, 'nantes': 6, 'ne': 7, 'manque': 8, 'jamais': 9, 'les': 10, 'cours': 11, 'machine': 12, 'learning!': 13, 'à': 14, 'centrale': 15, 'jeune': 16, 'mange': 17, 'pasta': 18}\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "print(mon_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNLZI-mFtkYE"
   },
   "source": [
    "> Afficher la liste des termes de ponctuations qui sont retirés par le Tokenizer. Modifier le fitre pour ne pas retirer le point d'exclamation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "4n2xQJJttkYH",
    "outputId": "075c7a92-8928-4f87-f4c9-6a5bf8acfec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "print(mon_tokenizer.filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35kXP_2itkYP"
   },
   "source": [
    "> Transformer maintenant les mots en listes d'entiers avec la méthode `texts_to_sequences()` de la classe Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E6StTuVStkYQ",
    "outputId": "e75aeb58-59cb-452c-d897-abf81efd7d56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 5, 3, 4, 6], [1, 7, 8, 9, 10, 11, 4, 12, 13], [1, 2, 3, 14, 15], [1, 2, 16], [1, 17, 18]]\n"
     ]
    }
   ],
   "source": [
    "sequences = mon_tokenizer.texts_to_sequences(samples)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHFrtLqBtkYX"
   },
   "source": [
    "# Word Embeddings (plongement des mots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7rcQuoztkYY"
   },
   "source": [
    "Il existe deux façons d'obtenir des embeddings de mots:\n",
    "\n",
    "- On peut apprendre un plongement pour une tache bien précise en amont (comme la classification des documents ou la prédiction des sentiments). Dans ce cas, on apprend le plongement comme on le fait pour un réseau de neurone classique.\n",
    "\n",
    "- On peut utiliser un embedding qui a été pré-entrainé pour une autre tâche, et que l'on \"recycle\" ici pour représenter les mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FDM9jQXtkYZ"
   },
   "source": [
    "### Apprentissage du plongement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpFEBhphtkYc"
   },
   "source": [
    "> En consultant la documentation sur la couche [`Embedding` de Keras](https://keras.io/layers/embeddings/), indiquer quels paramètres faut-il donner en argument à la fonction `Embedding` pour que celle-ci puisse représenter un plongement dans un espace de dimension 64 de séquences de longeur 10 mots dans corpus de 1000 mots retenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "DiMZrn9GtkYd",
    "outputId": "1f4aa5d7-43bd-4467-8089-cc7a9d4bc10d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64, input_length=10)### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXrINO1CtkYh"
   },
   "source": [
    "La couche `Embedding` prend en entrée un tenseur 2D d'entiers, de taille  `nombre de séquences` x  `longueur d'une séquence`.\n",
    "\n",
    "Toutes les séquences dans un bacth (de séquences) doivent donc avoir la même longueur, quitte à tronquer ou compléter avec des zeros les séquences trop longues ou trop courtes.\n",
    "\n",
    "Cette couche renvoie un tenseur 3D de valeurs numériques de taille `nombre de séquences` x  `longueur d'une séquence` x `dim d'arrivée du plongement`. \n",
    "\n",
    "Ces tenseurs 3D  peuvent ensuite être connectés à des couches récurrentes ou convolutionnelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgKde-WgtkYh"
   },
   "source": [
    "Dans un réseau de neurones, nous allons maintenant créer une première couche de plongement (embedding layer) et nous allons apprendre les poids de ce plongement exactement comme on le fait pour une couche dense.  Nous allons pour cela utiliser les données [imdb newswires Reuters](https://keras.io/datasets/#reuters-newswire-topics-classification) qui peuvent être directement chargées dans keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yKRnp7gMtkYi",
    "outputId": "4830d804-7b61-4633-dad5-3653c86d4970"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "max_mots = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_mots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XSpeleYtkYl"
   },
   "source": [
    "L'argument `num_words` correspond au nombre maximal de mots utilisés comme features. On le limite ici à 10000.\n",
    "\n",
    "> Vérifier que les mots ont été chargés sous la forme d'entiers. Que représente ici y ? Quel est l'objectif de ce problème d'apprentissage ? On parle \"d'analyse de sentiment\" (sentiment analysis ou opinion mining). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L3LuqLF3tkYl",
    "outputId": "072c5639-0872-4b72-c80a-bd7a5ab4d860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re: Representer les commentaires écrites prédire si une commentaire représente une émotion positive ou négitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYmkYuVrtkYo"
   },
   "source": [
    "> Bonus : retrouver les phrases à partir des vecteurs d'entiers (voir la doc de `imdb.load_dat`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTPAayRttkYo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "index_from=3\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+index_from) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in x_train[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0W8nqgotkYs"
   },
   "source": [
    "> Utiliser la fonction [preprocessing.sequence.pad_sequences](https://keras.io/api/preprocessing/timeseries/#padsequences-function) pour transformer `x_train` et `x_test` en deux tenseurs 2D de tailles `nb de sequences` x  `long max d une sequence = 20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s0S7qsz-tkYs",
    "outputId": "1f0bd33d-cf99-4824-eda7-aa7ec1ec25e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 20)\n",
      "(25000, 20)\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train_padded = pad_sequences(x_train, maxlen=20)\n",
    "x_test_padded = pad_sequences(x_test, maxlen=20)\n",
    "\n",
    "print (x_train_padded.shape)\n",
    "print (x_test_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raVGkng6tkYu"
   },
   "source": [
    "> Construire un réseau à propagation avant comme suit:\n",
    "- Une couche d'embedding qui plonge chaque mot dans un espace de dimension 8.\n",
    "- Une couche Flatten pour redimensionner le tenseur 3D des plongements en un tenseur 2D  de taille `nb de sequences` x  (8*20)  \n",
    "- Une couche dense avec activation sigmoid pour la classification finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "0Br819HBtkYv",
    "outputId": "50541480-1a21-4506-8bd1-e76adb38c9e8"
   },
   "outputs": [],
   "source": [
    "### TO DO ###\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.models import Model\n",
    "\n",
    "input = Input(shape=(20, ))\n",
    "x = Embedding(max_mots, 8, input_length=20)(input)\n",
    "x = Flatten()(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=input, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-RejtiAtkYx"
   },
   "source": [
    "> Utiliser un optimiseur `rmsprop` avec perte `binary_crossentropy` et suivi de la métrique `acc` (précision) le long de la trajectoire d'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "GZV3J0HHtkYy",
    "outputId": "6187d5de-f5c0-4b3a-ac34-059823397a45"
   },
   "outputs": [],
   "source": [
    "### TO DO ###\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1C5JLdxtkY0"
   },
   "source": [
    "> Affichez le résumé du réseau de neurones ainsi construit et assurez-vous que vous comprenez les dimensions affichées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "h7jle7NftkY0",
    "outputId": "ce7d01d1-707b-440b-d746-f71faab4d5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3bzGP3AtkY2"
   },
   "source": [
    "> Ajuster le modèle sur les données d'apprentissage et donner la précision de validation finale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "GABhvI-AtkY2",
    "outputId": "0a13e1a6-91c7-47e3-9109-a9ac944b53ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.6452 - acc: 0.6502 - val_loss: 0.5677 - val_acc: 0.7228\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.5050 - acc: 0.7588 - val_loss: 0.4945 - val_acc: 0.7552\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.4487 - acc: 0.7902 - val_loss: 0.4800 - val_acc: 0.7666\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.4212 - acc: 0.8054 - val_loss: 0.4765 - val_acc: 0.7693\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.4009 - acc: 0.8183 - val_loss: 0.4785 - val_acc: 0.7723\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3841 - acc: 0.8286 - val_loss: 0.4825 - val_acc: 0.7706\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.3678 - acc: 0.8362 - val_loss: 0.4875 - val_acc: 0.7685\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.3515 - acc: 0.8469 - val_loss: 0.4958 - val_acc: 0.7652\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.3357 - acc: 0.8554 - val_loss: 0.5012 - val_acc: 0.7631\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.3195 - acc: 0.8641 - val_loss: 0.5091 - val_acc: 0.7602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2eede811c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TO DO ###\n",
    "model.fit(x_train_padded, \n",
    "          y_train, \n",
    "          epochs=10, \n",
    "          batch_size=32, \n",
    "          validation_data=(x_test_padded,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nJhiFCHYtkY5"
   },
   "source": [
    "Le taux de bien classés tourne autour de 75%, ce qui est correct, mais on peut espérer faire mieux en utilisant le caractère \"séquentiel\" des phrases, grâce à des réseaux récurrents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlRRtQBftkY5"
   },
   "source": [
    "# Construction d'un réseau récurrent simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "339UmcyytkY6"
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9m0qT6d4tkY7"
   },
   "source": [
    "Une couche `SimpleRNN` prend en entrée un tenseur 3D de taille `batch_size` x `timesteps` (longeur de la séquence) x  `input_features` (typiquement la dimension de l'embedding). \n",
    "\n",
    "Comme tous les modèles récurrents, `SimpleRNN` peut renvoyer la suite complète de toutes les sorties pour chaque temps (le long de la séquence), ou bien tout simplement la denière sortie pour chaque séquence. \n",
    "\n",
    "> Expliquer la différence de dimension observée sur la couche récurrente dans les deux architectures proposées ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "zxzlwjjMtkY8",
    "outputId": "21751960-988e-4f4a-a686-7672e6466ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 10, 32)            32000     \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,080\n",
      "Trainable params: 34,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = 1000,output_dim=32,input_length = 10))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "4e9H80STtkY9",
    "outputId": "c02abeeb-81cb-4871-bf88-38ae0f4407c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 10, 32)            32000     \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 10, 32)            2080      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,080\n",
      "Trainable params: 34,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = 1000,output_dim=32,input_length = 10))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re: Différence : retourner la dernière sortie de la séquence de sortie, ou la séquence complète."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URasM4qDtkY_"
   },
   "source": [
    "> Préparer des données lexicales d'apprentissage et de test pour les données [`imdb`](https://keras.io/api/datasets/imdb/#load_data-function) selon les spécifications suivantes:\n",
    "- nombre de mots pris en compte : 10000 \n",
    "- longeur maximale des séquences : 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "L8R_jhZAtkY_",
    "outputId": "15be8d17-b515-4f86-b34b-f8a2651ca1f8"
   },
   "outputs": [],
   "source": [
    "### TO DO ###\n",
    "max_mots = 10000\n",
    "max_len = 500\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_mots, maxlen = max_len)\n",
    "x_train_padded = pad_sequences(x_train, maxlen=500)\n",
    "x_test_padded = pad_sequences(x_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwjsjJgdtkZB"
   },
   "source": [
    "> Construire un réseau à propagation avant comme suit:\n",
    "- Une couche d'embedding qui plonge chaque mot dans un espace de dimension 32.\n",
    "- Une couche `SimpleRNN` avec uniquement sortie finale\n",
    "- Une couche dense avec activation sigmoid pour la classification finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV0RW75ttkZC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 1)                 34        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,036\n",
      "Trainable params: 320,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_mots ,output_dim=32, input_length = max_len))\n",
    "model.add(SimpleRNN(1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNn_c4LMtkZD"
   },
   "source": [
    "> Utiliser un optimiseur `rmsprop` avec perte `binary_crossentropy` et suivi de la métrique `acc` (précision) le long de la trajectoire d'optimisation. Ajuster le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cbUbqektkZE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "716/716 [==============================] - 277s 382ms/step - loss: 0.6870 - acc: 0.5372 - val_loss: 0.6804 - val_acc: 0.5571\n",
      "Epoch 2/5\n",
      "716/716 [==============================] - 254s 355ms/step - loss: 0.6584 - acc: 0.6090 - val_loss: 0.6767 - val_acc: 0.5634\n",
      "Epoch 3/5\n",
      "716/716 [==============================] - 254s 355ms/step - loss: 0.6340 - acc: 0.6433 - val_loss: 0.6842 - val_acc: 0.5685\n",
      "Epoch 4/5\n",
      "716/716 [==============================] - 254s 355ms/step - loss: 0.6163 - acc: 0.6654 - val_loss: 0.6938 - val_acc: 0.5700\n",
      "Epoch 5/5\n",
      "716/716 [==============================] - 254s 355ms/step - loss: 0.6042 - acc: 0.6799 - val_loss: 0.7002 - val_acc: 0.5693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2eeb2b6730>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TO DO ###\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train_padded, y_train, epochs=5, batch_size=32, validation_data=(x_test_padded,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5W0vrlNGwBgg"
   },
   "source": [
    "# Construction d'un réseau récurrent avec cellules LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_dK3HRtwhak"
   },
   "source": [
    "> Construire enfin un réseau similaire où vous aurez remplacé la couche SimpleRNN par une couche [LSTM](https://keras.io/layers/recurrent/#lstm).\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "6nSBWWW2wgv1",
    "outputId": "e7d7eeb6-09d9-4d17-fc4c-315807b9dffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,138\n",
      "Trainable params: 320,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "from keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_mots ,output_dim=32, input_length = max_len))\n",
    "model.add(LSTM(1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716/716 [==============================] - 328s 451ms/step - loss: 0.6631 - acc: 0.6350 - val_loss: 0.5802 - val_acc: 0.7821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2ee7ecd910>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train_padded, y_train, epochs=1, batch_size=32, validation_data=(x_test_padded,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06AUej2ttkZU"
   },
   "source": [
    "# Utilisation d'un embedding pré-entrainé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DI1PFMdtkZV"
   },
   "source": [
    "Cette fois nous allons partir des données `Imdb` brutes et plonger celles-ci dans un espace via un plongement qui a déjà été ajusté (sur des données différentes et pour un problème autre). Nous allons utiliser [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove) : télécharger glove.6B.zip (près d'un giga !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Wa3SLdDtkZV"
   },
   "source": [
    "> Télécharger les données brutes à [cette adresse](http://mng.bz/0tIo). Les textes positifs et négatifs sont classés dans des repertoires de même nom. Compléter le code ci-dessous pour importer et préparer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = '../../../DATA/aclImdb/train/' ### TO DO ###\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['pos', 'neg']:\n",
    "    dir_name = os.path.join(data_path, label_type )\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJFHlNCItkZX",
    "outputId": "59fa292a-4360-490d-95c0-773eb3d9b566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Px8rgO_TtkZa"
   },
   "source": [
    "> Effectuer les opérations de traitement lexical (tokenization) pour un corpus de 10000 mots et des séquence de mots d'une longueur maximale de 100 mots. Transformer `labels` en un vecteur `numpy`. Vérifier les dimensions des objets construits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###\n",
    "max_words = 10000\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words=max_words)\n",
    "token.fit_on_texts(texts)\n",
    "x_train_seq = token.texts_to_sequences(texts)\n",
    "x_train = pad_sequences(x_train_seq, maxlen=max_len)\n",
    "y_train = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 100)\n",
      "y_train shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM1r5wmctkZw"
   },
   "source": [
    "> Extraire 1000 données pour l'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FH37Eu7tkZw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1000\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "num_samples = 1000\n",
    "data = x_train[:num_samples]\n",
    "labels = y_train[:num_samples]\n",
    "print('Number of samples:', len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeltkAkhtkZx"
   },
   "source": [
    "> En suivant la documentation de Keras sur cette [page](https://keras.io/examples/nlp/pretrained_word_embeddings/), utiliser  un embedding de type Glove sur les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUdFEwuStkZy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "path_to_glove_file = '../../../DATA/glove.6B/glove.6B.100d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = token.word_index\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwVzJ3NptkZ0"
   },
   "source": [
    "> Utiliser cet embedding pour construire des réseaux recurrents ou non pour prédire la sortie Y. Evaluer la précision de vos modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# Créer une matrice de poids à partir de l'embedding Glove\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 11s 182ms/step - loss: 0.6838 - accuracy: 0.5888 - val_loss: 0.9994 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 3s 112ms/step - loss: 0.6528 - accuracy: 0.6250 - val_loss: 0.9847 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 3s 108ms/step - loss: 0.6445 - accuracy: 0.6313 - val_loss: 1.0190 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 3s 113ms/step - loss: 0.6342 - accuracy: 0.6225 - val_loss: 0.9485 - val_accuracy: 0.0300\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 3s 107ms/step - loss: 0.6230 - accuracy: 0.6562 - val_loss: 1.0419 - val_accuracy: 0.0350\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 3s 102ms/step - loss: 0.6162 - accuracy: 0.6562 - val_loss: 1.1922 - val_accuracy: 0.0100\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 2s 99ms/step - loss: 0.5991 - accuracy: 0.6862 - val_loss: 1.0990 - val_accuracy: 0.0850\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 3s 109ms/step - loss: 0.5970 - accuracy: 0.6925 - val_loss: 1.1130 - val_accuracy: 0.0950\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 3s 103ms/step - loss: 0.5765 - accuracy: 0.7113 - val_loss: 0.8213 - val_accuracy: 0.3550\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 3s 102ms/step - loss: 0.5658 - accuracy: 0.7063 - val_loss: 0.8508 - val_accuracy: 0.3500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2eb19a06d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer un modèle récurrent avec une couche d'embedding Glove\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train[:1000], y_train[:1000], epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 38ms/step - loss: 0.6042 - accuracy: 0.6580\n",
      "Accuracy: 65.799999\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(data, labels)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDRxcABAtkZ5"
   },
   "source": [
    "> Utiliser cet embedding de mots pour évaluer la proximité entre quelques phrases que vous choisirez. Vous pourrez représenter les données dans le premier plan factoriel d'une ACP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exemples de phrases\n",
    "sentences = samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour transformer une phrase en un vecteur moyenné\n",
    "def sentence_to_vector(sentence, embeddings_index):\n",
    "    words = sentence.split()\n",
    "    vectors = [embeddings_index.get(word, np.zeros((100,))) for word in words]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Transformer les phrases en vecteurs moyens\n",
    "vectors = [sentence_to_vector(sentence, embeddings_index) for sentence in sentences]\n",
    "\n",
    "# Réduire la dimensionnalité en deux dimensions avec l'ACP\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualiser les phrases dans le premier plan factoriel\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "for i, sentence in enumerate(sentences):\n",
    "    plt.annotate(sentence, (vectors_2d[i, 0], vectors_2d[i, 1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN-NLP_correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7673fce31fe4afe50b4d7357ba4e3f000f877c9b2250741ca11e5d63029f695"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
