{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0259d7e1-c217-4927-8a76-f1dd090a6822",
   "metadata": {},
   "source": [
    "# The Nim game\n",
    "\n",
    "![Nim](http://auxboissauvages.fr/auboissauvages/jeux/geants/images/jeu_de_Nim.png)\n",
    "\n",
    "## Outline\n",
    "- [Part 1:  Formalizing the problem](#1)\n",
    "- [Part 2:  Q-learning](#2)\n",
    "- [Part 3:  SARSA learning](#3)\n",
    "\n",
    "In this lab, we will try to apply the learning strategy we learnt during lecture to learn how to win at the Nim game. The Nim game consists in placing $N$ small objects on the table and two players will play in turn by picking up to 3 of these objects and putting them aside. Of course you cannot pick more objects than those still on the table.\n",
    "\n",
    "The player who took all the remaining object wins!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab032d-a8ee-438d-9297-49d25215ae45",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# Part 1: Formalizing the problem\n",
    "\n",
    "- Find 8 small objects, e.g., pencils, and play a game with the person at your left. (It there's noboby at your left, remember that human beings are social animals ;-)\n",
    "- What are the state and action spaces for this game?\n",
    "- Without having resort to the algorithm presented in the course, according to you, what are the states with the largest values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc495a1-fe24-40e3-bf9c-4ec67b538e6a",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "# Part 2: Q-learning\n",
    "\n",
    "- Write a function that learns how to win at Nim using an (espsilon-greedy) Q-learning strategy. *Note that for this purpose of learning there is no need to have 2 separate players, only one player will play since our goal is to learn how to win and not to actually play a game. I have written a small function for you as well: `getReward` which computes the reward for a given state*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821108dd-d29f-49e6-9169-131adcc85c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getReward(remaining):\n",
    "    return (remaining == 0) * 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263b6e5d-73f7-4ea2-a9e4-28eb835d6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/question4.py\n",
    "\n",
    "# Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dbe540-1a3f-4936-9489-754adf85c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/question4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e0cc2-e749-4379-91e7-5ace52f868a4",
   "metadata": {},
   "source": [
    "- Run your algorithm to estimate the Q-table and estimate the optimal policy. Does your result make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb8dbbc-92a3-4609-aa5e-edc6c3adcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/question5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6f8b7c-cbcc-401e-b685-12b9af4e4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/question5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f120c-3e6d-4fb2-86db-bbc9bb260dbc",
   "metadata": {},
   "source": [
    "- Try to change the learning rate, the exploration probability decay and see how they influence your results. You can also change the initial number of object placed on the table if you wish... *Note that it is common practice to set the exploration probability to 1 for the first `K` iterations and then decrease it slowly to 0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b121f01e-cc23-4143-b306-98618d5daf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0fec6-9b0c-4ea5-a2fd-63b05220de84",
   "metadata": {},
   "source": [
    "- Now you will play against your own learned policy. To this aim, I have written a small function for you where you should pass in your estimated optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee99143-f268-4516-a243-591882ce5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playNim(learned_policy):\n",
    "    nb_boulettes = len(learned_policy)\n",
    "    \n",
    "    ## Which player will start playing?\n",
    "    player = np.random.randint(0, 2)\n",
    "    \n",
    "    while nb_boulettes > 0:\n",
    "        if (player % 2) == 0:\n",
    "            ## Our IA is playing\n",
    "            action = learned_policy[nb_boulettes-1]\n",
    "        else:\n",
    "            print(\"Table:\", [\"I\"] * nb_boulettes)\n",
    "            \n",
    "            notValid = True\n",
    "            while notValid or action == -1:\n",
    "                action = int(input(\"How many? (-1 to abort)\"))\n",
    "                notValid = action > np.min((3, nb_boulettes))\n",
    "                \n",
    "                if action == -1:\n",
    "                    return \"Game aborted\"\n",
    "        \n",
    "        nb_boulettes -= action\n",
    "        player += 1\n",
    "    \n",
    "    if (player % 2) == 0:\n",
    "        winner = \"You win\"\n",
    "    else:\n",
    "        winner = \"You loose\"\n",
    "    \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c548a169-213f-4ba9-8b6b-9dbc65c52498",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c307ead-18ce-48ee-bb80-c314e7f617d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: ['I', 'I', 'I', 'I', 'I', 'I', 'I', 'I']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many? (-1 to abort) -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Game aborted'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playNim(learned_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60844f-2704-492b-9b49-40aaec207451",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "# Part 3: SARSA learning (Optional)\n",
    "\n",
    "- Redo the analysis done in Part 2 but now using a SARSA learning strategy. Before implementing your SARSA algorithm try to figure out why it is now necessary to have two players. To this purpose I have written a small dumb `AI` for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8755a201-e201-4878-b584-b39780dd3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AI(remaining):\n",
    "    return remaining - np.random.randint(1, 1 + np.min((3, remaining)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "106187c3-e435-45e0-91b8-de7b8c510c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n",
    "\n",
    "# %load solutions/sarsa.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e6fcf-6196-4ba6-bc30-f08ec51c87b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
