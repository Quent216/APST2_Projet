{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pygame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84bbe6b0",
   "metadata": {},
   "source": [
    "## Part 2 : REINFORCE algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f1aef6f",
   "metadata": {},
   "source": [
    "For the second part of this project, we are going to work on the **Pendulum**. \n",
    "\n",
    "This environment is part of the Ici ce qui suit [Classic Control environments](https://gymnasium.farama.org/environments/classic_control/).\n",
    "\n",
    "Please read that page first for general information.\n",
    "\n",
    "![Ceci est un exemple d’image](https://gymnasium.farama.org/_images/pendulum.gif)\n",
    "   \n",
    "    | Action space     | Box(-2.0, 2.0, (1,), float32) |\n",
    "    |Observation Shape | (3,)                          |\n",
    "    |Observation High  | [1. 1. 8.]                    |\n",
    "    |Observation low   | [-1. -1. -8.]                 |\n",
    "    |Import            | gymnasium.make(\"Pendulum-v1\") |\n",
    "\n",
    "### Description \n",
    "\n",
    "The inverted pendulum swingup problem is based on the classic problem in control theory. The system consists of a pendulum attached at one end to a fixed point, and the other end being free. The pendulum starts in a random position and the goal is to apply torque on the free end to swing it into an upright position, with its center of gravity right above the fixed point.\n",
    "\n",
    "The diagram below specifies the coordinate system used for the implementation of the pendulum’s dynamic equations.\n",
    "\n",
    "![Ceci est un exemple d’image](https://gymnasium.farama.org/_images/pendulum.png)\n",
    "\n",
    "* x-y: cartesian coordinates of the pendulum’s end in meters.\n",
    "\n",
    "* theta: angle in radians.\n",
    "\n",
    "* tau: torque in N.m. Defined as positive *counter-clockwise*.\n",
    "\n",
    "### Action space \n",
    "\n",
    "The action is a ndarray with shape (1,) representing the torque applied to free end of the pendulum.\n",
    "\n",
    "\n",
    "    | Num | Action | Min  | Max |\n",
    "    |-----|--------|------|-----|\n",
    "    | 0   | Torque | -2.0 | 2.0 |\n",
    "    \n",
    "### Observation space \n",
    "\n",
    "The observation is a ndarray with shape (3,) representing the x-y coordinates of the pendulum’s free end and its angular velocity.\n",
    "\n",
    "    | Num | Observation      | Min  | Max |\n",
    "    |-----|------------------|------|-----|\n",
    "    | 0   | x = cos(theta)   | -1.0 | 1.0 |\n",
    "    | 1   | y = sin(theta)   | -1.0 | 1.0 |\n",
    "    | 2   | Angular Velocity | -8.0 | 8.0 |\n",
    "\n",
    "### Rewards \n",
    "\n",
    "he reward function is defined as:\n",
    "\n",
    "$ r = -(theta^2 + 0.1 * theta_{dt}^2 + 0.001*torque^2) $\n",
    "\n",
    "where $theta$ is the pendulum’s angle normalized between $[-\\pi, \\pi]$ (with 0 being in the upright position). Based on the above equation, the minimum reward that can be obtained is $-(\\pi^2 + 0.1 * 8^2 + 0.001 * 2^2) = -16.2736044$, while the maximum reward is zero (pendulum is upright with zero velocity and no torque applied).\n",
    "\n",
    "### Starting state \n",
    "\n",
    "The starting state is a random angle in $[-\\pi, \\pi]$ and a random angular velocity in [-1,1].\n",
    "\n",
    "### Episode Truncation\n",
    "The episode truncates at 200 time steps.\n",
    "\n",
    "### Arguments\n",
    "* g: acceleration of gravity measured in $(m.s^{-2})$ used to calculate the pendulum dynamics. The default value is g = 10.0 .\n",
    "\n",
    "On reset, the options parameter allows the user to change the bounds used to determine the new random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81, render_mode = \"human\")\n",
    "print(f'Number of possible actions: {env.action_space} that correspond to the torque applied to the free end of the pendulum (positive counter-clock wise)')\n",
    "print(f'Number of states: {env.observation_space} that correspond to the x-y coordinates (Cartesian basis) and the angular velocity of the free end of the pendulum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_init(x):\n",
    "    theta=[0]*len(x)\n",
    "    return np.array(theta)\n",
    "\n",
    "def policy(action,state,theta):\n",
    "    p=state @ theta\n",
    "    q=1/(1+np.exp(-p))\n",
    "    if action==0:\n",
    "        return 1-q\n",
    "    else : \n",
    "        return q\n",
    "def gradient_function(action,state,theta):\n",
    "    if action==0:\n",
    "        return -state*policy(1,state,theta)\n",
    "    else :\n",
    "        return state*policy(0,state,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = []\n",
    "\n",
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, s_size=3, h_size=20):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(s_size, h_size)\n",
    "\n",
    "        self.mu = torch.nn.Linear(h_size, 1)\n",
    "        self.sigma = torch.nn.Linear(h_size, 1)\n",
    "\n",
    "        self.distribution = torch.distributions.Normal\n",
    "        \n",
    "        torch.nn.init.constant_(self.sigma.bias,0.1)\n",
    "        \n",
    "        \n",
    "    # output : (mean , var) \n",
    "    # note : var>0\n",
    "    def forward(self, state):\n",
    "        state = torch.Tensor(state)\n",
    "        x = torch.nn.functional.relu(self.fc1(state))\n",
    "        # x = self.fc2(x)\n",
    "\n",
    "        mu = self.mu(x)[0]\n",
    "        sigma = torch.nn.functional.softplus(self.sigma(x)[0]) + 0.025\n",
    "        m = self.distribution(mu, sigma)\n",
    "        \n",
    "        # print(sigma.item())\n",
    "        sigmas.append(sigma.item())\n",
    "\n",
    "        a = m.sample()\n",
    "        log_prob = m.log_prob(a)\n",
    "\n",
    "        return a,log_prob\n",
    "\n",
    "policy = Policy(h_size=50)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15545418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(theta_0,lr,gamma,n_episode):\n",
    "    env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "    theta=theta_0\n",
    "    \n",
    "    for i in range(n_episode):\n",
    "        X=[] #list of states\n",
    "        A=[] #list of actions\n",
    "        R=[] #list of rewards\n",
    "        x,_=env.reset()\n",
    "        n_move = 0 \n",
    "        terminated=False\n",
    "        \n",
    "        while not terminated: #episode to fill the lists\n",
    "            if n_move > 500:\n",
    "                env.close()\n",
    "                pygame.display.quit()\n",
    "                pygame.quit()\n",
    "                raise Exception(\"Too many attempts, failed\")\n",
    "            n_move += 1\n",
    "            X.append(x)\n",
    "            \n",
    "            u= np.random.uniform(0,1)\n",
    "            if u<=policy(1,x,theta):\n",
    "                action=1\n",
    "            else :\n",
    "                action=0\n",
    "            A.append(action)\n",
    "            x, r, terminated, truncated, info = env.step([action])\n",
    "            R.append(r)\n",
    "        \n",
    "        print(\"Total rewards :\",np.sum(R))\n",
    "        n=0 \n",
    "        while n<n_move: #list run for the adjustment of theta\n",
    "          \n",
    "            G=0\n",
    "            for i in range(n+1,n_move):\n",
    "                G=G+gamma**(i-n-1)*R[i]\n",
    "                \n",
    "            grad=gradient_function(A[n],X[n],theta)\n",
    "            theta=theta+lr*gamma**n*G*grad\n",
    "            \n",
    "            n += 1\n",
    "    env.close()\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "n_episode=100\n",
    "gamma=1\n",
    "theta_0=[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce(theta_0,lr,gamma,n_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_episode):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "    x,info=env.reset()\n",
    "    n_move=0\n",
    "    action = env.action_space.sample()\n",
    "    while not terminated:\n",
    "        if n_move > 10000:\n",
    "            raise Exception(\"Too many attempts, failed\")\n",
    "        n_move += 1\n",
    "        X.append(x)\n",
    "        policy=policyparam(action,x,theta)\n",
    "        action = np.argmax(policy)\n",
    "        A.append(action)\n",
    "        x, reward, terminated, truncated, info = env.step(action)\n",
    "        print(reward)\n",
    "        R.append(reward)\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
